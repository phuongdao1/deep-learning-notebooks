{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LeNet on MNIST Dataset Using Multiple GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/cpu:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 16562518561336613333, name: \"/gpu:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 67108864\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 7913761063815264308\n",
       " physical_device_desc: \"device: 0, name: GRID K520, pci bus id: 0000:00:03.0\", name: \"/gpu:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 67108864\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 10782718246840404907\n",
       " physical_device_desc: \"device: 1, name: GRID K520, pci bus id: 0000:00:04.0\", name: \"/gpu:2\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 67108864\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 4133597402929815830\n",
       " physical_device_desc: \"device: 2, name: GRID K520, pci bus id: 0000:00:05.0\", name: \"/gpu:3\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 67108864\n",
       " locality {\n",
       "   bus_id: 1\n",
       " }\n",
       " incarnation: 16752761831954155716\n",
       " physical_device_desc: \"device: 3, name: GRID K520, pci bus id: 0000:00:06.0\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Go over the resources on the current system\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "Image Shape: (28, 28, 1)\n",
      "\n",
      "Training Set:   55000 samples\n",
      "Validation Set: 5000 samples\n",
      "Test Set:       10000 samples\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=False)\n",
    "X_train, y_train           = mnist.train.images, mnist.train.labels\n",
    "X_validation, y_validation = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test             = mnist.test.images, mnist.test.labels\n",
    "\n",
    "assert(len(X_train) == len(y_train))\n",
    "assert(len(X_validation) == len(y_validation))\n",
    "assert(len(X_test) == len(y_test))\n",
    "\n",
    "print()\n",
    "print(\"Image Shape: {}\".format(X_train[0].shape))\n",
    "print()\n",
    "print(\"Training Set:   {} samples\".format(len(X_train)))\n",
    "print(\"Validation Set: {} samples\".format(len(X_validation)))\n",
    "print(\"Test Set:       {} samples\".format(len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add padding to the MNIST dataset as MNIST used 28x28x1 images while LeNet architecture only accepts 32x32xC images, where C is the number of color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pad images with 0s\n",
    "X_train      = np.pad(X_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_validation = np.pad(X_validation, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "X_test       = np.pad(X_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 512\n",
    "num_gpus = 4\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LeNet Architecture\n",
    "\n",
    "![LeNet Architecture](lenet.png)\n",
    "Source: Yan LeCun\n",
    "\n",
    "Here we setup the LeNet Model such that all the variables are on the CPU. Later on, we will reuse the same variables on each GPU to compute gradients. The CPU then does averaging gradients returned from each GPU and update the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNetModel(images):    \n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            conv1_W = tf.get_variable(\"conv1_W\",initializer=tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "            conv1_b = tf.get_variable(\"conv1_b\",initializer=tf.zeros(6))\n",
    "        conv1   = tf.nn.conv2d(images, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "        conv1 = tf.nn.relu(conv1, name=scope.name)\n",
    "\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='pool1')\n",
    "\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            conv2_W = tf.get_variable(\"conv2_W\",initializer=tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "            conv2_b = tf.get_variable(\"conv2_b\",initializer=tf.zeros(16))\n",
    "        conv2   = tf.nn.conv2d(pool1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "\n",
    "        conv2 = tf.nn.relu(conv2, name=scope.name)\n",
    "\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID', name='pool2')\n",
    "\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "        fc0   = flatten(pool2)\n",
    "\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            fc1_W = tf.get_variable(\"fc1_W\",initializer=tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))\n",
    "            fc1_b = tf.get_variable(\"fc1_b\",initializer=tf.zeros(120))\n",
    "\n",
    "        fc1    = tf.nn.relu(tf.matmul(fc0, fc1_W) + fc1_b, name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            fc2_W  = tf.get_variable(\"fc2_W\",initializer=tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "            fc2_b  = tf.get_variable(\"fc2_b\",initializer=tf.zeros(84))\n",
    "\n",
    "        fc2    = tf.nn.relu(tf.matmul(fc1, fc2_W) + fc2_b, name=scope.name)\n",
    "\n",
    "    with tf.variable_scope('fc3') as scope:\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            fc3_W  = tf.get_variable(\"fc3_W\",initializer=tf.truncated_normal(shape=(84, 10), mean = mu, stddev = sigma))\n",
    "            fc3_b  = tf.get_variable(\"fc3_b\",initializer=tf.zeros(10))\n",
    "        logits = tf.add(tf.matmul(fc2, fc3_W), fc3_b, name=scope.name)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Averaging Gradients and Make Parallel Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the loss using cross entropy on outputed softmax probabilities\n",
    "\n",
    "def loss(logits, labels):\n",
    "    one_hot_y = tf.one_hot(labels, 10)\n",
    "    cross_entropy_all = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)    \n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy_all, name='cross_entropy_mean')\n",
    "    \n",
    "    return cross_entropy_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the function to average the gradients returned from the GPU\n",
    "# this function is taken from Tensorflow tutorial\n",
    "\n",
    "def average_gradients(tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        grad = tf.concat(0, grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "        \n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a function split the parameters of a function fn into num_gpus chunks and for each chunk, build an operation on each GPU \n",
    "# and finally gather the results and return it \n",
    "# this function has been adapted from EffectiveTensorflow\n",
    "\n",
    "def make_parallel(fn, num_gpus, **kwargs):\n",
    "  input_splits = {}\n",
    "  for k, v in kwargs.items():\n",
    "    input_splits[k] = tf.split(0, num_gpus, v)\n",
    "\n",
    "  output_splits = []\n",
    "  for i in range(num_gpus):\n",
    "    with tf.device(\"/gpu:\" + str(i)):\n",
    "      with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "        output_splits.append(fn(**{k : v[i] for k, v in input_splits.items()}))\n",
    "\n",
    "  return tf.concat(0, output_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X_train, y_train):\n",
    "    with tf.Graph().as_default(), tf.device(\"/cpu:0\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "        whole_batch_x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "        whole_batch_y = tf.placeholder(tf.int32, (None))\n",
    "\n",
    "        images_per_gpu = tf.split(0, num_gpus, whole_batch_x)\n",
    "        labels_per_gpu = tf.split(0, num_gpus, whole_batch_y)\n",
    "        \n",
    "        tower_grads = []\n",
    "        with tf.variable_scope(tf.get_variable_scope()):\n",
    "            for gpu_idx in range(num_gpus):\n",
    "                with tf.device(\"/gpu:\" + str(gpu_idx)):\n",
    "                    with tf.name_scope('%s_%d' % (\"tower_gpu\", gpu_idx)) as scope:\n",
    "                        with tf.variable_scope(\"same_on_each_device\", reuse=(gpu_idx>0)):\n",
    "                            loss_v = loss(LeNetModel(images_per_gpu[gpu_idx]), labels_per_gpu[gpu_idx])                        \n",
    "                            grads_and_vars = optimizer.compute_gradients(loss_v)\n",
    "                            tower_grads.append(grads_and_vars)\n",
    "        \n",
    "        avg_grad = average_gradients(tower_grads)\n",
    "        apply_grad_op = optimizer.apply_gradients(avg_grad)\n",
    "        \n",
    "        with tf.variable_scope(\"same_on_each_device\", reuse=True):\n",
    "            val_batch_x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "            val_batch_y = tf.placeholder(tf.int32, (None))\n",
    "            val_batch_one_hot_y = tf.one_hot(val_batch_y, 10)\n",
    "            val_batch_logits =  make_parallel(LeNetModel, num_gpus, images = val_batch_x)\n",
    "            correct_prediction = tf.equal(tf.argmax(val_batch_logits, 1), tf.argmax(val_batch_one_hot_y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        sess = tf.Session(config=tf.ConfigProto(\n",
    "            allow_soft_placement=True,\n",
    "            log_device_placement=True))\n",
    "        \n",
    "        sess.run(init)\n",
    "        print(\"Training...\")\n",
    "        print()\n",
    "        for i in range(num_epochs):\n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "            num_examples = len(X_train)\n",
    "            max_steps = num_examples // (num_gpus*batch_size)\n",
    "            for offset in range(0, max_steps):\n",
    "                end = offset + num_gpus*batch_size\n",
    "                batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "                _ = sess.run([apply_grad_op], feed_dict={whole_batch_x: batch_x, whole_batch_y: batch_y})\n",
    "            \n",
    "            num_examples = len(X_validation)\n",
    "            total_accuracy = 0\n",
    "            for offset in range(0, num_examples, batch_size):\n",
    "                batch_x, batch_y = X_validation[offset:offset+batch_size], y_validation[offset:offset+batch_size]\n",
    "                acc = sess.run(accuracy, feed_dict={val_batch_x: batch_x, val_batch_y: batch_y})\n",
    "                total_accuracy += (acc * len(batch_x))\n",
    "            validation_accuracy = total_accuracy / num_examples\n",
    "\n",
    "            print(\"EPOCH {} ...\".format(i+1))\n",
    "            print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.811\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.914\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "train(X_train, y_train)\n",
    "end = time.time()\n",
    "print(\"it takes %f (s)\" % (end-start))\n",
    "\n",
    "## It took around 17s to train on GPUs compared to 42s on one single GPU"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
